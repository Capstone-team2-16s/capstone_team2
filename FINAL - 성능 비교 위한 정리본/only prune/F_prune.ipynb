{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F-prune.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC3t6mk8d3tc"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ALft_YAdwYv"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame as df\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"english\")\n",
        "from  nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model #모델 저장\n",
        "\n",
        "#sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import time #수행시간 측정\n",
        "from google.colab import files #colab에 모델 save,load\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# emoji\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "#Colab\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNYG73mgeYDy"
      },
      "source": [
        "drive.mount('/gdrive', force_remount = True) # drive.mount('/content/gdrive') #,force_remount=True\n",
        "my_path = '/gdrive/My Drive/Colab Notebooks/' # my_path='/content/gdrive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRqjyqMzeDPj"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5AGOdZFeCl-"
      },
      "source": [
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "MAX_LEN = 50\n",
        "VOCAB_SIZE = 400000\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.35, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO2M6FZ4gc_y"
      },
      "source": [
        "미리 학습된 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMpqbOqen_J"
      },
      "source": [
        "tk = Tokenizer(num_words=VOCAB_SIZE)\n",
        "\n",
        "with open(my_path+'wordIndex.json') as json_file:\n",
        "    word_index = json.load(json_file)\n",
        "    tk.word_index = word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqxuERxFgXQ-"
      },
      "source": [
        "prun80 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RYSz4AFfxp6"
      },
      "source": [
        "model = load_model(my_path+'pruned80_tCNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuJp7RC9vnt9"
      },
      "source": [
        "분석 결과 라벨링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvNZFwuEgTrj"
      },
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score < SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score > SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FoSXY_AgTQ4"
      },
      "source": [
        "def predict(ex_text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    x_encoded = tk.texts_to_sequences([ex_text])\n",
        "    res_test=np.array(pad_sequences(x_encoded, maxlen=MAX_LEN, padding='post'))\n",
        "    # Predict\n",
        "    score = model.predict([res_test])\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    \n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGXOaU5BgLqw"
      },
      "source": [
        "# 트윗 데이터로 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPQsB4fafx4f"
      },
      "source": [
        "class preproc_Sentence:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def readTweets(request_id):\n",
        "        id = request_id\n",
        "        file_name = 'twitter_'\n",
        "        fileformat = '.txt'\n",
        "        filename = file_name + id + fileformat\n",
        "\n",
        "        data_path = my_path+'data/'\n",
        "\n",
        "        # 분석 요청된 유명인 트윗 파일 open\n",
        "        with open(data_path + filename, 'r', encoding = \"utf-8\") as f:\n",
        "            tweets = pd.read_csv(f, sep = \"\\n\", names = ['data'])\n",
        "        f.close()\n",
        "\n",
        "        return tweets\n",
        "\n",
        "    def preprocTweets(tweets):        \n",
        "        # URL 변환\n",
        "        tweets['data'] = tweets['data'].replace(to_replace = \"((www\\.[^\\s]+)|(http?://[^\\s]+)|(https?://[^\\s]+))\", value = \"URL \", regex = True)\n",
        "        # 소문자 변환\n",
        "        tweets['preprocess'] = tweets['data'].str.lower()\n",
        "        # @ 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"'@[^\\s]+\", value = \"USERID\", regex = True)\n",
        "        # hashtag 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"#([^\\s]+)\", value = \"HASHTAG\", regex = True)\n",
        "        # hashtag 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)\", value = \"EMAIL\", regex = True)\n",
        "       \n",
        "        # Emoji 변환\n",
        "        tweets_raw = tweets['preprocess']\n",
        "\n",
        "        for i in range(len(tweets_raw)):\n",
        "            tweets_raw[i] = emoji.demojize(tweets_raw[i], use_aliases = True)\n",
        "\n",
        "        tweets['preprocess'] = tweets_raw\n",
        "\n",
        "        return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm2QWzPHfyFw"
      },
      "source": [
        "class preproc_Word:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def readTweet(request_id):\n",
        "        id = request_id\n",
        "        file_name = 'twitter_'\n",
        "        fileformat = '.txt'\n",
        "        filename = file_name + id + fileformat\n",
        "\n",
        "        data_path = my_path+'data/'\n",
        "\n",
        "        # 분석 요청된 유명인 트윗 파일 open\n",
        "        with open(data_path + filename, 'r', encoding = \"utf-8\") as file:\n",
        "            tweet = file.read()\n",
        "       \n",
        "        return tweet\n",
        "\n",
        "    def preprocWordTweet(tweet):\n",
        "        # 소문자 변환\n",
        "        tweet = tweet.lower()\n",
        "        # URL 제거\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(http?://[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
        "        # 구두점 제거\n",
        "        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "        # 숫자 제거\n",
        "        tweet = re.sub('\\s[0-9]+', '', tweet)\n",
        "        # 아이디 제거\n",
        "        tweet = re.sub('@[A-Za-z0-9]+', '', tweet)\n",
        "        # 이메일 제거\n",
        "        tweet = re.sub('([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)', '', tweet)\n",
        "\n",
        "        return tweet\n",
        "    \n",
        "    def tokenizeWord(tweet):\n",
        "        word_tokens = word_tokenize(tweet)\n",
        " \n",
        "        res = []\n",
        "        for w in word_tokens: \n",
        "            if w not in stop_words: \n",
        "                res.append(w)\n",
        "        return res\n",
        "    \n",
        "    def stemmerWord(res):\n",
        "        words = [stemmer.stem(w) for w in res]\n",
        "        return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdHJKZZ2fx74"
      },
      "source": [
        "class word_COUNT:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def countWord(request_id):\n",
        "        tweet = preproc_Word.readTweet(request_id)\n",
        "        tweet = preproc_Word.preprocWordTweet(tweet)\n",
        "        res = preproc_Word.tokenizeWord(tweet)\n",
        "        words = preproc_Word.stemmerWord(res)\n",
        "        print('자주 사용하는 단어 TOP5')\n",
        "        print(Counter(words).most_common(n=5))\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13klvWYcfx02"
      },
      "source": [
        "class tweet_SentimentAnalyse :\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def sentimentAnalyse(tweets_data) :\n",
        "        # 결과 dataframe 생성\n",
        "        df_res = pd.DataFrame({'text':[], 'label':[], 'score':[], 'elapsed_time':[]})\n",
        "        for col,item in tweets_data.iterrows():\n",
        "            # predict class로 수정 필요\n",
        "            res = predict(item[1])\n",
        "            df_res.loc[col] = [item[0], res['label'], res['score'],res['elapsed_time'] ]\n",
        "        return df_res\n",
        "\n",
        "    def countTypes(df_res):\n",
        "        # 전체 수 계산\n",
        "        df_res['label'].value_counts()\n",
        "        # 타입별 비율 계산\n",
        "        print('트윗 문장 감정 비율')\n",
        "        print(df_res['label'].value_counts(normalize=True).mul(100).round(2).astype(str)+'%')\n",
        "        print('POSITIVE')\n",
        "        print(df_res.sort_values(by=\"score\", ascending=False).head(2))\n",
        "        print('NEGATIVE')\n",
        "        print(df_res.sort_values(by=\"score\", ascending=True).head(2))\n",
        "        print('NEUTRAL')\n",
        "        df_res['cal'] = abs(df_res['score'] - 0.5)\n",
        "        print(df_res.sort_values(by=\"cal\", ascending=True).head(2))\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdY3Wts_q_1l"
      },
      "source": [
        "트위터 계정 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqQwqGFvq9xH"
      },
      "source": [
        "@AdinaPorter\n",
        "@aliciakeys\n",
        "@AllyBrooke\n",
        "@altonbrown\n",
        "@AnneMarie\n",
        "@Ashton5SOS\n",
        "@barbarastarrcnn\n",
        "@BebeRexha\n",
        "@iambeckyg\n",
        "@BigSean\n",
        "@BillGates\n",
        "@chancetherapper\n",
        "@charlieputh\n",
        "@ChrisEvans\n",
        "@ClintSmithIII\n",
        "@DamonGupton\n",
        "@DanRather\n",
        "@DojaCat\n",
        "@DUALIPA\n",
        "@DwyaneWade\n",
        "@TheEllenShow\n",
        "@elliegoulding\n",
        "@elonmusk\n",
        "@GretchenCarlson\n",
        "@IGGYAZALEA\n",
        "@jameelajamil\n",
        "@JaredDudley619\n",
        "@jason_mraz\n",
        "@jelani9\n",
        "@Acosta\n",
        "@jimcramer\n",
        "@hitRECordJoe\n",
        "@BBCkatyaadler\n",
        "@Kehlani\n",
        "@KimKardashian\n",
        "@KingJames\n",
        "@liamgallagher\n",
        "@LukasGraham\n",
        "@MariahCarey\n",
        "@marshmellomusic\n",
        "@megynkelly\n",
        "@NiallOfficial\n",
        "@Pink\n",
        "@ParisHilton\n",
        "@Rjeff24\n",
        "@robreiner\n",
        "@RobertDowneyJr\n",
        "@StephenKing\n",
        "@tim_cook\n",
        "@Zedd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaTkPlB_kzyb"
      },
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okddsVjRfxeO"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 분석 아이디\n",
        "    request_id = '@BBCkatyaadler'\n",
        "    # 분석 시간 측정\n",
        "    start_at = time.time()\n",
        "    # 문장 전처리\n",
        "    preproc_Sentence()\n",
        "    tweets = preproc_Sentence.readTweets(request_id) # web에서는 실행 X\n",
        "    tweets_data = preproc_Sentence.preprocTweets(tweets)\n",
        "    # 트위터 감정 분석 및 2문장 제공\n",
        "    df_res = tweet_SentimentAnalyse.sentimentAnalyse(tweets_data)\n",
        "    tweet_SentimentAnalyse.countTypes(df_res)\n",
        "    print()\n",
        "    # 단어 전처리\n",
        "    preproc_word = preproc_Word() # web에서는 실행 X\n",
        "    # 단어 카운트\n",
        "    word_COUNT.countWord(request_id)\n",
        "    print(\"소요시간\", time.time()-start_at)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}