{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_cnn_1109","provenance":[{"file_id":"1Iu0IiyS0Z0Zf2oDjDSRf5bTRyUBkfsh9","timestamp":1604899905107}],"collapsed_sections":[],"authorship_tag":"ABX9TyPFFHhkAyImhTqgYjX4GEXl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oVtGh-4bDpIn"},"source":["# Import Library"]},{"cell_type":"code","metadata":{"id":"v0AJaVyPCRys","executionInfo":{"status":"ok","timestamp":1604900775607,"user_tz":-540,"elapsed":5640,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}},"outputId":"3d1d6aef-1059-4dd7-87ae-020f8861fbe8","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas as pd\n","from pandas import DataFrame as df\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# nltk\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from  nltk.stem import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","\n","#Keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model #모델 저장\n","\n","#sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Utility\n","import re\n","import numpy as np\n","import time #수행시간 측정\n","from google.colab import files #colab에 모델 save,load\n","from collections import Counter\n","\n","# emoji\n","!pip install emoji\n","import emoji"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n","\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=0282418830c4cab2de0fc5ef148d3549123dd90702335715e43070da20fae32f\n","  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2iooLul4Db5c","executionInfo":{"status":"ok","timestamp":1604900213113,"user_tz":-540,"elapsed":625,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["# =============== 셋팅 =============== #\n","\n","# DATASET\n","DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","TRAIN_SIZE = 0.8\n","MAX_LEN = 50\n","VOCAB_SIZE = 400000\n","\n","# TEXT CLENAING\n","TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n","\n","#전처리\n","stop_words = stopwords.words(\"english\")\n","stemmer = SnowballStemmer(\"english\")\n","\n","# KERAS\n","SEQUENCE_LENGTH = 300\n","EPOCHS = 8\n","BATCH_SIZE = 1024\n","\n","# SENTIMENT\n","POSITIVE = \"POSITIVE\"\n","NEGATIVE = \"NEGATIVE\"\n","NEUTRAL = \"NEUTRAL\"\n","SENTIMENT_THRESHOLDS = (0.4, 0.7)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"52gAw7EyDfYe","executionInfo":{"status":"ok","timestamp":1604899951648,"user_tz":-540,"elapsed":34154,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}},"outputId":"d035f48c-84a6-404e-8a81-93452bde0da0","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Colab에 연결해서 사용하기\n","from google.colab import drive\n","drive.mount('/content/gdrive') #,force_remount=True\n","my_path='/content/gdrive/My Drive/Colab Notebooks/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JK6hPwoBDnPh"},"source":["# 모델"]},{"cell_type":"markdown","metadata":{"id":"wP7JXxVAjw70"},"source":["미리 학습된 토크나이저 불러오기"]},{"cell_type":"code","metadata":{"id":"4b-TnFy-NV7k","executionInfo":{"status":"ok","timestamp":1604900179101,"user_tz":-540,"elapsed":1187,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["tk = Tokenizer(num_words=VOCAB_SIZE)\n","\n","import json\n","\n","with open(my_path+'wordIndex.json') as json_file:\n","  word_index = json.load(json_file)\n","  tk.word_index = word_index"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXxJP6bgjzA5"},"source":["모델 만들기"]},{"cell_type":"code","metadata":{"id":"XCGc8zctKAmw","executionInfo":{"status":"ok","timestamp":1604900065729,"user_tz":-540,"elapsed":677,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["def make_model():\n","\n","  model = Sequential()\n","\n","  model.add(Embedding(vocab_size, 32, input_length=max_len))\n","  model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Dropout(0.2))\n","  model.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Dropout(0.2))\n","  model.add(Conv1D(filters=32, kernel_size=7, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Dropout(0.2))\n","  model.add(Conv1D(filters=32, kernel_size=8, padding='same', activation='relu'))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Dropout(0.2))\n","  model.add(Flatten())\n","  model.add(Dense(1, input_shape=(1,)))\n","  model.compile('SGD','mse',metrics=['accuracy'])\n","  model.summary() \n","\n","make_model()    \n","model.fit(X_train, y_train, epochs=10, verbose=1)\n","model.save('model.h5')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVLNnwaQGcqY"},"source":["text-CNN 모델 로드"]},{"cell_type":"code","metadata":{"id":"X_5MXv5AOCy6","executionInfo":{"status":"ok","timestamp":1604900131842,"user_tz":-540,"elapsed":2198,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["model = load_model(my_path+'text-CNN.h5')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcNH3SqYIwRB","executionInfo":{"status":"ok","timestamp":1604900181980,"user_tz":-540,"elapsed":595,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["def decode_sentiment(score, include_neutral=True):\n","    if include_neutral:        \n","        label = NEUTRAL\n","        if score <= SENTIMENT_THRESHOLDS[0]:\n","            label = NEGATIVE\n","        elif score >= SENTIMENT_THRESHOLDS[1]:\n","            label = POSITIVE\n","\n","        return label\n","    else:\n","        return NEGATIVE if score < 0.5 else POSITIVE"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLC_MSjVHq--","executionInfo":{"status":"ok","timestamp":1604900218788,"user_tz":-540,"elapsed":648,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["def predict(ex_text, include_neutral=True):\n","    start_at = time.time()\n","    x_encoded = tk.texts_to_sequences([ex_text])\n","    res_test=np.array(pad_sequences(x_encoded, maxlen=MAX_LEN, padding='post'))\n","    # Predict\n","    score = model.predict([res_test])\n","    # Decode sentiment\n","    label = decode_sentiment(score, include_neutral=include_neutral)\n","    \n","    return {\"label\": label, \"score\": float(score),\n","       \"elapsed_time\": time.time()-start_at}"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"nm2nRzjnHuh_","executionInfo":{"status":"ok","timestamp":1604900221483,"user_tz":-540,"elapsed":925,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}},"outputId":"e126d53d-cfeb-40d5-9b57-4676f40cb1c5","colab":{"base_uri":"https://localhost:8080/"}},"source":["predict(\"I love it\")"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'elapsed_time': 0.4421391487121582,\n"," 'label': 'POSITIVE',\n"," 'score': 0.8468796610832214}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"XeTF_kX6JVqH"},"source":["# 트위터 데이터로 예측해보기"]},{"cell_type":"code","metadata":{"id":"THu_EHdIBPpd","executionInfo":{"status":"ok","timestamp":1604900517520,"user_tz":-540,"elapsed":808,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["class preproc_Sentence:\n","    def __init__(self):\n","        pass\n","\n","    def readTweets(request_id):\n","        id = request_id\n","        file_name = 'twitter_'\n","        fileformat = '.txt'\n","        filename = file_name + id + fileformat\n","\n","        data_path = my_path+'data/'\n","\n","        # 분석 요청된 유명인 트윗 파일 open\n","        with open(data_path + filename, 'r', encoding = \"utf-8\") as f:\n","            tweets = pd.read_csv(f, sep = \"\\n\", names = ['data'])\n","        f.close()\n","\n","        return tweets\n","\n","    def preprocTweets(tweets):        \n","        # URL 변환\n","        tweets['data'] = tweets['data'].replace(to_replace = \"((www\\.[^\\s]+)|(https?://[^\\s]+))\", value = \"URL \", regex = True)\n","        # 소문자 변환\n","        tweets['preprocess'] = tweets['data'].str.lower()\n","        # @ 변환\n","        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"'@[^\\s]+\", value = \"USERID\", regex = True)\n","        # hashtag 변환\n","        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"#([^\\s]+)\", value = \"HASHTAG\", regex = True)\n","        # hashtag 변환\n","        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)\", value = \"EMAIL\", regex = True)\n","       \n","        # Emoji 변환\n","        tweets_raw = tweets['preprocess']\n","\n","        for i in range(len(tweets_raw)):\n","            tweets_raw[i] = emoji.demojize(tweets_raw[i], use_aliases = True)\n","\n","        tweets['preprocess'] = tweets_raw\n","\n","        return tweets"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hJ7wSX6hD85","executionInfo":{"status":"ok","timestamp":1604900522659,"user_tz":-540,"elapsed":667,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["class preproc_Word:\n","    def __init__(self):\n","        pass\n","\n","    def readTweet(request_id):\n","        id = request_id\n","        file_name = 'twitter_'\n","        fileformat = '.txt'\n","        filename = file_name + id + fileformat\n","\n","        data_path = my_path+'data/'\n","\n","        # 분석 요청된 유명인 트윗 파일 open\n","        with open(data_path + filename, 'r', encoding = \"utf-8\") as file:\n","            tweet = file.read()\n","       \n","        return tweet\n","\n","    def preprocWordTweet(tweet):\n","        # 소문자 변환\n","        tweet = tweet.lower()\n","        # 구두점 제거\n","        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","        # URL 제거\n","        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n","        # 숫자 제거\n","        tweet = re.sub('\\s[0-9]+', '', tweet)\n","        # 아이디 제거\n","        tweet = re.sub('@[A-Za-z0-9]+', '', tweet)\n","        # 이메일 제거\n","        tweet = re.sub('([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)', '', tweet)\n","\n","        return tweet\n","    \n","    def tokenizeWord(tweet):\n","        stop_words = set(stopwords.words('english')) \n","        word_tokens = word_tokenize(tweet)\n"," \n","        res = []\n","        for w in word_tokens: \n","            if w not in stop_words: \n","                res.append(w)\n","        return res\n","    \n","    def stemmerWord(res):\n","        stemmer = SnowballStemmer('english')\n","        words = [stemmer.stem(w) for w in res]\n","        return words"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROuTXACBhGnH","executionInfo":{"status":"ok","timestamp":1604900525203,"user_tz":-540,"elapsed":599,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["class word_COUNT:\n","    def __init__(self):\n","        pass\n","\n","    def countWord(request_id):\n","        tweet = preproc_Word.readTweet(request_id)\n","        tweet = preproc_Word.preprocWordTweet(tweet)\n","        res = preproc_Word.tokenizeWord(tweet)\n","        words = preproc_Word.stemmerWord(res)\n","        print('자주 사용하는 단어 TOP5')\n","        print(Counter(words).most_common(n=5))\n","        print()"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzypZgB8hJMw","executionInfo":{"status":"ok","timestamp":1604900527084,"user_tz":-540,"elapsed":623,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}}},"source":["class tweet_SentimentAnalyse :\n","    def __init__(self):\n","        pass\n","\n","    def sentimentAnalyse(tweets_data) :\n","        # 결과 dataframe 생성\n","        df_res = pd.DataFrame({'text':[], 'label':[], 'score':[], 'elapsed_time':[]})\n","        print('트윗 문장 감정 분석 결과')\n","        for col,item in tweets_data.iterrows():\n","            # predict class로 수정 필요\n","            res = predict(item[1])\n","            df_res.loc[col] = [item[0], res['label'], res['score'],res['elapsed_time'] ]\n","        print(df_res)\n","        print()\n","        return df_res\n","\n","    def countTypes(df_res):\n","        # 전체 수 계산\n","        df_res['label'].value_counts()\n","        # 타입별 비율 계산\n","        print('트윗 문장 감정 비율')\n","        print(df_res['label'].value_counts(normalize=True).mul(100).round(2).astype(str)+'%')\n","        print()"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"KE7VgxnyhJr5","executionInfo":{"status":"ok","timestamp":1604901175685,"user_tz":-540,"elapsed":12276,"user":{"displayName":"Nani Yang","photoUrl":"","userId":"13035183620485579542"}},"outputId":"2b91bfe0-94de-4016-a4b8-3a03c541d373","colab":{"base_uri":"https://localhost:8080/"}},"source":["if __name__ == \"__main__\":\n","    # 분석 아이디\n","    request_id = '@AdinaPorter'\n","    # 문장 전처리\n","    preproc_Sentence()\n","    tweets = preproc_Sentence.readTweets(request_id)\n","    tweets_data = preproc_Sentence.preprocTweets(tweets)\n","    # 트위터 감정 분석\n","    df_res = tweet_SentimentAnalyse.sentimentAnalyse(tweets_data)\n","    tweet_SentimentAnalyse.countTypes(df_res)\n","    print()\n","    # 단어 전처리\n","    preproc_word = preproc_Word()\n","    # 단어 카운트\n","    word_COUNT.countWord(request_id)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["트윗 문장 감정 분석 결과\n","                                                  text  ... elapsed_time\n","0                                             Maybe...  ...     0.045038\n","1    I forgot... Happy Birthday to everyone born in...  ...     0.039138\n","2                                            🎂🍾🍦🍨🎁🎈🎉🛍   ...     0.040243\n","3                        Also don’t forget to #VOTE !!  ...     0.037968\n","4    Is Red Crescent the best charity to donate mon...  ...     0.043840\n","..                                                 ...  ...          ...\n","295             I’ll be watching at 8 PM Pacific Time.  ...     0.034677\n","296  Wanted to thank Director Ian Samoil for allowi...  ...     0.033079\n","297         Just heard the news about #BreonnaTaylor .  ...     0.032254\n","298  It physically pains me so I cannot imagine how...  ...     0.032758\n","299                                  #BlackLivesMatter  ...     0.032119\n","\n","[300 rows x 4 columns]\n","\n","트윗 문장 감정 비율\n","POSITIVE    59.67%\n","NEGATIVE    20.67%\n","NEUTRAL     19.67%\n","Name: label, dtype: object\n","\n","\n","자주 사용하는 단어 TOP5\n","[('the100', 50), ('hair', 23), ('thank', 20), ('watch', 17), ('vote', 15)]\n","\n"],"name":"stdout"}]}]}