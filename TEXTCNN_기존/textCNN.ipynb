{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwAE1e6HI0Ta"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4XmT0MkKSGd"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRYFp-9oJleV"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame as df\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "from gensim.models import Word2Vec #@\n",
        "from gensim.utils import simple_preprocess #@\n",
        "from gensim.models.keyedvectors import KeyedVectors #@\n",
        "\n",
        "#Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model #모델 저장\n",
        "\n",
        "#sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import time #수행시간 측정\n",
        "from google.colab import files #colab에 모델 save,load\n",
        "\n",
        "# emoji패키지 설치\n",
        "# 영구 설치 가능, https://sikaleo.tistory.com/m/78?category=932203\n",
        "!pip install emoji\n",
        "import emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4i4PiNdJn-o"
      },
      "source": [
        "# =============== 셋팅 =============== #\n",
        "\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "#전처리\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBNYLnKJKZkf"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqhpE8m_Joq3"
      },
      "source": [
        "my_path = '/gdrive/My Drive/colab/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uff7dhRyJvoz"
      },
      "source": [
        "#학습데이터 로드\n",
        "dataset=pd.read_csv(my_path+'train.csv',encoding = DATASET_ENCODING, names=DATASET_COLUMNS)\n",
        "print(dataset.shape) #1600000,6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRHQE15aJwMM"
      },
      "source": [
        "dataset.head() # negative:0, positive:4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9SNxfXNKdCU"
      },
      "source": [
        "디코더 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOCnMeKnJwVB"
      },
      "source": [
        "decode_map = {0: NEGATIVE, 2: NEUTRAL, 4: POSITIVE} #숫자 => 분류 문장\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV5XX9-LJwTi"
      },
      "source": [
        "%%time\n",
        "dataset.target = dataset.target.apply(lambda x: decode_sentiment(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1QXEs1EKgzL"
      },
      "source": [
        "# Pre-Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0mubiMKj1S"
      },
      "source": [
        "클리닝 텍스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eul8spEbJwRp"
      },
      "source": [
        "#학습 데이터 텍스트 전처리\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxw_Hyg_J2Kc"
      },
      "source": [
        "#학습 데이터 전처리 진행\n",
        "dataset.text = dataset.text.apply(lambda x: preprocess(x)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FhltznMKn9Q"
      },
      "source": [
        "학습 데이터 나누기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-HScoUAJ7hQ"
      },
      "source": [
        "train, test = train_test_split(dataset, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(train))\n",
        "print(\"TEST size:\", len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hmoorJ7kF"
      },
      "source": [
        "documents = [_text.split() for _text in train.text] #list, 1280000*50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmX1V4QGJ7my"
      },
      "source": [
        "vocab_size = 400000\n",
        "tk = Tokenizer(num_words=vocab_size)\n",
        "tk.fit_on_texts(train.text) \n",
        "x_train = tk.texts_to_sequences(train.text)\n",
        "x_test = tk.texts_to_sequences(test.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4IGVd2hJ7pq"
      },
      "source": [
        "labels = train.target.unique().tolist() #POSITIVE NEUTRAL NEGATIVE\n",
        "labels.append(NEUTRAL)\n",
        "print(labels)\n",
        "\n",
        "encoder = LabelEncoder() #문장 -> 숫자 자동으로\n",
        "encoder.fit(train.target.tolist())\n",
        "\n",
        "y_train = encoder.transform(train.target.tolist())\n",
        "y_test = encoder.transform(test.target.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1) #1열로 자동으로 만들어줍니다.\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSgucG1UKtym"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x541BqwCJ7s6"
      },
      "source": [
        "max_len=max(len(l) for l in x_train) #한 문장에서 최대 단어 개수를 반환 #max_len=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CsEtZjJ7wa"
      },
      "source": [
        "X_train = np.array(pad_sequences(x_train, maxlen=max_len, padding='post')) #max_len만큼 padding 값 설정 \n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2e91pAK1hM"
      },
      "source": [
        "text-CNN 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJrT8hmkKyD1"
      },
      "source": [
        "model = load_model(my_path+'text-CNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNDLM8R6Kx7-"
      },
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEqA7RIK5DV"
      },
      "source": [
        "def predict(ex_text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    x_encoded = tk.texts_to_sequences([ex_text])\n",
        "    res_test=np.array(pad_sequences(x_encoded, maxlen=max_len, padding='post'))\n",
        "    # Predict\n",
        "    score = model.predict([res_test])\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    \n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UORdWEJ2K6s8"
      },
      "source": [
        "predict(\"That's so sad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI6I1UJsK7bU"
      },
      "source": [
        "# 트위터 데이터로 예측해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrexAVQ9J2TS"
      },
      "source": [
        "# 트윗 텍스트 전처리\n",
        "def preproc_tweets(request_id):\n",
        "    #사용자에게 분석 요청 받은 유명인 아이디\n",
        "    id = request_id\n",
        "\n",
        "    file_name = 'tweets_'\n",
        "    fileformat = '.txt'\n",
        "    filename = file_name + id + fileformat\n",
        "\n",
        "    data_path = '/gdrive/My Drive/colab/data/'\n",
        "\n",
        "    # 분석 요청된 유명인 트윗 파일 open\n",
        "    with open(data_path + filename, 'r', encoding = \"utf-8\") as f:\n",
        "        tweets = pd.read_csv(f, sep = \"\\n\", names = ['data'])\n",
        "    f.close()\n",
        "\n",
        "    #소문자 변환\n",
        "    tweets['preprocess'] = tweets['data'].str.lower()\n",
        "\n",
        "    #@,#제거\n",
        "    tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"(@|#)\", value = \" \", regex = True)\n",
        "\n",
        "    #Emoji 변환\n",
        "    tweets_raw = tweets['preprocess']\n",
        "\n",
        "    for i in range(len(tweets_raw)):\n",
        "        tweets_raw[i] = emoji.demojize(tweets_raw[i], use_aliases = True)\n",
        "\n",
        "    tweets['preprocess'] = tweets_raw\n",
        "\n",
        "    return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUv-QWIOK8zN"
      },
      "source": [
        "# DB에서 분석할 트위터 텍스트 파일 가져오기\n",
        "# 입력 값 ID 필요\n",
        "request_id = \"@AnneMarie\"\n",
        "\n",
        "tweet_data = preproc_tweets(request_id)\n",
        "\n",
        "tweet_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqHieyuGK-pu"
      },
      "source": [
        "df_res = pd.DataFrame({'text':[],\n",
        "                   'label':[],\n",
        "                   'score':[],\n",
        "                  'elapsed_time':[]}) #결과 dataframe 생성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-H_z1xELBIO"
      },
      "source": [
        "for col,item in tweet_data.iterrows():\n",
        "  res=predict(item[1])\n",
        "  df_res.loc[col]=[item[0], res['label'], res['score'],res['elapsed_time'] ]\n",
        "\n",
        "df_res"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}