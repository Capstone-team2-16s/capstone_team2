{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwAE1e6HI0Ta",
        "outputId": "1d909290-1196-4317-f8f2-dd27302ae60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4XmT0MkKSGd"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRYFp-9oJleV",
        "outputId": "87b2a265-dc23-4f03-c232-3b03f7e55dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "from gensim.models import Word2Vec #@\n",
        "from gensim.utils import simple_preprocess #@\n",
        "from gensim.models.keyedvectors import KeyedVectors #@\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model # 모델 저장\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame as df\n",
        "import time # 수행시간 측정\n",
        "from google.colab import files # colab에 모델 save,load\n",
        "from collections import Counter\n",
        "\n",
        "# emoji\n",
        "!pip install emoji\n",
        "import emoji"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=66167a76aa7878dbd10948228325687906a63c1c144f224ce814d9e3fa3a1f50\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4i4PiNdJn-o"
      },
      "source": [
        "# =============== 셋팅 =============== #\n",
        "\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "#전처리\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBNYLnKJKZkf"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqhpE8m_Joq3"
      },
      "source": [
        "my_path = '/gdrive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uff7dhRyJvoz",
        "outputId": "7d22282a-7fdc-4033-f830-00149966080f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#학습데이터 로드\n",
        "dataset=pd.read_csv(my_path+'train.csv',encoding = DATASET_ENCODING, names=DATASET_COLUMNS)\n",
        "print(dataset.shape) #1600000,6"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600000, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRHQE15aJwMM",
        "outputId": "b7377dec-db6f-45c7-a755-6fafc17b491f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "dataset.head() # negative:0, positive:4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                               text\n",
              "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  ...  is upset that he can't update his Facebook by ...\n",
              "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3       0  ...    my whole body feels itchy and like its on fire \n",
              "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9SNxfXNKdCU"
      },
      "source": [
        "디코더 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOCnMeKnJwVB"
      },
      "source": [
        "decode_map = {0: NEGATIVE, 2: NEUTRAL, 4: POSITIVE} #숫자 => 분류 문장\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV5XX9-LJwTi",
        "outputId": "cf3f0440-de6c-472f-ffcd-528cc4ca48d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "dataset.target = dataset.target.apply(lambda x: decode_sentiment(x))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 511 ms, sys: 0 ns, total: 511 ms\n",
            "Wall time: 515 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1QXEs1EKgzL"
      },
      "source": [
        "# Pre-Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0mubiMKj1S"
      },
      "source": [
        "클리닝 텍스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eul8spEbJwRp"
      },
      "source": [
        "# 학습 데이터 텍스트 전처리\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxw_Hyg_J2Kc"
      },
      "source": [
        "# 학습 데이터 전처리 진행\n",
        "dataset.text = dataset.text.apply(lambda x: preprocess(x)) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FhltznMKn9Q"
      },
      "source": [
        "학습 데이터 나누기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-HScoUAJ7hQ",
        "outputId": "a9449774-d82c-4d73-b3f6-d7ba18c86e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train, test = train_test_split(dataset, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(train))\n",
        "print(\"TEST size:\", len(test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN size: 1280000\n",
            "TEST size: 320000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hmoorJ7kF"
      },
      "source": [
        "documents = [_text.split() for _text in train.text] #list, 1280000*50"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmX1V4QGJ7my"
      },
      "source": [
        "vocab_size = 400000\n",
        "tk = Tokenizer(num_words=vocab_size)\n",
        "tk.fit_on_texts(train.text) \n",
        "x_train = tk.texts_to_sequences(train.text)\n",
        "x_test = tk.texts_to_sequences(test.text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4IGVd2hJ7pq",
        "outputId": "46db9849-597a-4164-b58b-e3b10f57d408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels = train.target.unique().tolist() # POSITIVE NEUTRAL NEGATIVE\n",
        "labels.append(NEUTRAL)\n",
        "print(labels)\n",
        "\n",
        "encoder = LabelEncoder() # 문장 -> 숫자 자동으로\n",
        "encoder.fit(train.target.tolist())\n",
        "\n",
        "y_train = encoder.transform(train.target.tolist())\n",
        "y_test = encoder.transform(test.target.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1) # 1열로 자동으로 만들어줍니다.\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['POSITIVE', 'NEGATIVE', 'NEUTRAL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSgucG1UKtym"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x541BqwCJ7s6"
      },
      "source": [
        "max_len=max(len(l) for l in x_train) # 한 문장에서 최대 단어 개수를 반환 # max_len=50"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CsEtZjJ7wa",
        "outputId": "9be5943a-5408-425a-dbd1-5fd1e204d796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train = np.array(pad_sequences(x_train, maxlen=max_len, padding='post')) # max_len만큼 padding 값 설정 \n",
        "print(X_train.shape, y_train.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1280000, 50) (1280000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2e91pAK1hM"
      },
      "source": [
        "text-CNN 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJrT8hmkKyD1"
      },
      "source": [
        "model = load_model(my_path+'text-CNN.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNDLM8R6Kx7-"
      },
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEqA7RIK5DV"
      },
      "source": [
        "def predict(ex_text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    x_encoded = tk.texts_to_sequences([ex_text])\n",
        "    res_test=np.array(pad_sequences(x_encoded, maxlen=max_len, padding='post'))\n",
        "    # Predict\n",
        "    score = model.predict([res_test])\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    \n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UORdWEJ2K6s8",
        "outputId": "1bc905d8-d3de-4a65-aa2e-654de853c021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "predict(\"That's so sad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'elapsed_time': 0.6012370586395264,\n",
              " 'label': 'NEGATIVE',\n",
              " 'score': 0.05291527509689331}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI6I1UJsK7bU"
      },
      "source": [
        "# 트위터 데이터로 예측해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVaGegU00kn8"
      },
      "source": [
        "트위터 문장 감정 분석 - 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrexAVQ9J2TS"
      },
      "source": [
        "# 트윗 문장 전처리\n",
        "class preproc_Sentence:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def readTweets(request_id):\n",
        "        id = request_id\n",
        "        file_name = 'twitter_'\n",
        "        fileformat = '.txt'\n",
        "        filename = file_name + id + fileformat\n",
        "\n",
        "        data_path = '/gdrive/My Drive/Colab Notebooks/data/'\n",
        "\n",
        "        # 분석 요청된 유명인 트윗 파일 open\n",
        "        with open(data_path + filename, 'r', encoding = \"utf-8\") as f:\n",
        "            tweets = pd.read_csv(f, sep = \"\\n\", names = ['data'])\n",
        "        f.close()\n",
        "\n",
        "        return tweets\n",
        "\n",
        "    def preprocTweets(tweets):        \n",
        "        # URL 변환\n",
        "        tweets['data'] = tweets['data'].replace(to_replace = \"((www\\.[^\\s]+)|(https?://[^\\s]+))\", value = \"URL \", regex = True)\n",
        "        # 소문자 변환\n",
        "        tweets['preprocess'] = tweets['data'].str.lower()\n",
        "        # @ 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"'@[^\\s]+\", value = \"USERID\", regex = True)\n",
        "        # hashtag 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"#([^\\s]+)\", value = \"HASHTAG\", regex = True)\n",
        "        # hashtag 변환\n",
        "        tweets['preprocess'] = tweets['preprocess'].replace(to_replace = \"([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)\", value = \"EMAIL\", regex = True)\n",
        "       \n",
        "        # Emoji 변환\n",
        "        tweets_raw = tweets['preprocess']\n",
        "\n",
        "        for i in range(len(tweets_raw)):\n",
        "            tweets_raw[i] = emoji.demojize(tweets_raw[i], use_aliases = True)\n",
        "\n",
        "        tweets['preprocess'] = tweets_raw\n",
        "\n",
        "        return tweets"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RzRoKEk0qFW"
      },
      "source": [
        "트위터 단어 카운트 - 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0c7sXbIzzTu"
      },
      "source": [
        "class preproc_Word:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def readTweet(request_id):\n",
        "        id = request_id\n",
        "        file_name = 'twitter_'\n",
        "        fileformat = '.txt'\n",
        "        filename = file_name + id + fileformat\n",
        "\n",
        "        data_path = '/gdrive/My Drive/Colab Notebooks/data/'\n",
        "\n",
        "        # 분석 요청된 유명인 트윗 파일 open\n",
        "        with open(data_path + filename, 'r', encoding = \"utf-8\") as file:\n",
        "            tweet = file.read()\n",
        "       \n",
        "        return tweet\n",
        "\n",
        "    def preprocWordTweet(tweet):\n",
        "        # 소문자 변환\n",
        "        tweet = tweet.lower()\n",
        "        # 구두점 제거\n",
        "        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "        # URL 제거\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
        "        # 숫자 제거\n",
        "        tweet = re.sub('\\s[0-9]+', '', tweet)\n",
        "        # 아이디 제거\n",
        "        tweet = re.sub('@[A-Za-z0-9]+', '', tweet)\n",
        "        # 이메일 제거\n",
        "        tweet = re.sub('([a-zA-Z0-9_.+-]@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-]+)', '', tweet)\n",
        "\n",
        "        return tweet\n",
        "    \n",
        "    def tokenizeWord(tweet):\n",
        "        stop_words = set(stopwords.words('english')) \n",
        "        word_tokens = word_tokenize(tweet)\n",
        " \n",
        "        res = []\n",
        "        for w in word_tokens: \n",
        "            if w not in stop_words: \n",
        "                res.append(w)\n",
        "        return res\n",
        "    \n",
        "    def stemmerWord(res):\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in res]\n",
        "        return words"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ3tRqnvLjhM"
      },
      "source": [
        "단어 카운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpM2mqDbHyu_"
      },
      "source": [
        "class word_COUNT:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def countWord(request_id):\n",
        "        tweet = preproc_Word.readTweet(request_id)\n",
        "        tweet = preproc_Word.preprocWordTweet(tweet)\n",
        "        res = preproc_Word.tokenizeWord(tweet)\n",
        "        words = preproc_Word.stemmerWord(res)\n",
        "        print('자주 사용하는 단어 TOP5')\n",
        "        print(Counter(words).most_common(n=5))\n",
        "        print()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UosCzeKdU2EB"
      },
      "source": [
        "트윗 감정 분석하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBbd8_GPAUE"
      },
      "source": [
        "class tweet_SentimentAnalyse :\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def sentimentAnalyse(tweets_data) :\n",
        "        # 결과 dataframe 생성\n",
        "        df_res = pd.DataFrame({'text':[], 'label':[], 'score':[], 'elapsed_time':[]})\n",
        "        print('트윗 문장 감정 분석 결과')\n",
        "        for col,item in tweets_data.iterrows():\n",
        "            # predict class로 수정 필요\n",
        "            res = predict(item[1])\n",
        "            df_res.loc[col] = [item[0], res['label'], res['score'],res['elapsed_time'] ]\n",
        "        print(df_res)\n",
        "        print()\n",
        "        return df_res\n",
        "\n",
        "    def countTypes(df_res):\n",
        "        # 전체 수 계산\n",
        "        df_res['label'].value_counts()\n",
        "        # 타입별 비율 계산\n",
        "        print('트윗 문장 감정 비율')\n",
        "        print(df_res['label'].value_counts(normalize=True).mul(100).round(2).astype(str)+'%')\n",
        "        print()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253DJjDnJfBA"
      },
      "source": [
        "@AdinaPorter\n",
        "@aliciakeys\n",
        "@AnneMarie\n",
        "@BillGates\n",
        "@birdy\n",
        "@charlieputh\n",
        "@ChrisEvans\n",
        "@DanReynolds\n",
        "@DojaCat\n",
        "@DwyaneWade\n",
        "@elliegoulding\n",
        "@elonmusk\n",
        "@IGGYAZALEA\n",
        "@JaredDudley619\n",
        "@jason_mraz\n",
        "@jelani9\n",
        "@Kehlani\n",
        "@liamgallagher\n",
        "@LukasGraham\n",
        "@MariahCarey\n",
        "@marshmellomusic\n",
        "@NiallOfficial\n",
        "@ParisHilton\n",
        "@Pink\n",
        "@rihanna\n",
        "@RobertDowneyJr\n",
        "@robreiner\n",
        "@TheEllenShow\n",
        "@tim_cook\n",
        "@Zedd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB3uYd0X0twn"
      },
      "source": [
        "# MAIN 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLIsvViz9YH",
        "outputId": "b2b65661-3cfc-4569-843f-c65310515732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 분석 아이디\n",
        "    request_id = '@AdinaPorter'\n",
        "    # 문장 전처리\n",
        "    preproc_Sentence()\n",
        "    tweets = preproc_Sentence.readTweets(request_id)\n",
        "    tweets_data = preproc_Sentence.preprocTweets(tweets)\n",
        "    # 트위터 감정 분석\n",
        "    df_res = tweet_SentimentAnalyse.sentimentAnalyse(tweets_data)\n",
        "    tweet_SentimentAnalyse.countTypes(df_res)\n",
        "    print()\n",
        "    # 단어 전처리\n",
        "    preproc_word = preproc_Word()\n",
        "    # 단어 카운트\n",
        "    word_COUNT.countWord(request_id)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "트윗 문장 감정 분석 결과\n",
            "                                                  text  ... elapsed_time\n",
            "0    At times, duvet covers feel like more trouble ...  ...     0.397666\n",
            "1    Those times are the 15-20 minutes it takes to ...  ...     0.033865\n",
            "2    After that, you realize, it was worth the stru...  ...     0.033823\n",
            "3                      I’m reading the news. Horrible.  ...     0.032204\n",
            "4            Please send more and I will help amplify.  ...     0.032126\n",
            "..                                                 ...  ...          ...\n",
            "230             I’ll be watching at 8 PM Pacific Time.  ...     0.031347\n",
            "231  Wanted to thank Director Ian Samoil for allowi...  ...     0.032301\n",
            "232         Just heard the news about #BreonnaTaylor .  ...     0.032557\n",
            "233  It physically pains me so I cannot imagine how...  ...     0.032469\n",
            "234                                  #BlackLivesMatter  ...     0.032911\n",
            "\n",
            "[235 rows x 4 columns]\n",
            "\n",
            "트윗 문장 감정 비율\n",
            "POSITIVE    60.43%\n",
            "NEGATIVE    20.43%\n",
            "NEUTRAL     19.15%\n",
            "Name: label, dtype: object\n",
            "\n",
            "\n",
            "자주 사용하는 단어 TOP5\n",
            "[('the100', 50), ('hair', 23), ('thank', 15), ('stori', 12), ('watch', 11)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}